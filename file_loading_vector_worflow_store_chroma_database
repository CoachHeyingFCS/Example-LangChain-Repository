#IMPORTANT: This solution is not great and sometimes I had to ask the same question multiple times.
#I prioritized reability and making it similar to the other code in this library and lost some acccuracy/speed
#Feel free to explore some other options for creating the chain as this one doesn't use the vectors to the best of their ability

from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate  
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_chroma import Chroma
import os

#--Important Variables--#
PDF_PATH = "/workspaces/Example-LangChain-Repository/Understanding Modelfile in Ollama.pdf"
DB_DIR = "./sql_chroma_db"
MODEL_NAME = "test-llm"

#Setting up variables in my chain
model = ChatOllama(model= MODEL_NAME)
prompt = ChatPromptTemplate.from_template(
    """ 
    You are an AI assistant. Use ONLY the provided context to answer the question. 
    If the answer is not clearly and directly supported by the context, respond exactly with:
    "I don't have enough context to answer that."

    Do NOT make up facts or speculate.
    "You must base your answer ONLY on the provided context. 
    If you include any information from the context, you must reference the filename or page it came from. 
    If there is no relevant context, respond: 'I don't have enough context to answer that.'"

    Question:
    {input}

    Context:
    {context}

    Answer:
    """
)
parser = StrOutputParser()

#Invoking my chain
chain = prompt | model | parser

def loadPDF():
    loader = PyPDFLoader(PDF_PATH)
    pages = loader.load_and_split()
    return pages

def chunk_PDF(document):
    #Creates the tool that will split the pages into individual chunks based on the size of chunk you want
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size = 700,
        chunk_overlap = 200,
        length_function = len,
        add_start_index = True
    )
    #Does the splitting
    chunks = text_splitter.split_documents(document)
    print(f"Split {len(document)} pages into {len(chunks)} chunks")
    return chunks
    
def build_or_load_vectors(chunks):
    thisEmbedding = FastEmbedEmbeddings()
    if os.path.exists(DB_DIR) and os.listdir(DB_DIR):
        print("Loading existing vector store...")
        vector_store = Chroma(persist_directory=DB_DIR, embedding_function = thisEmbedding)
    else:
        vector_store = Chroma.from_documents(documents = chunks, embedding = thisEmbedding, presist_director = DB_DR)
    return vector_store

def format_documents(sections):
    outputString = ""
    for s in sections:
        outputString += s.page_content
    return outputString

def ask(query, retriever):
    #Get the relevant chunks of document from the vectors
    document_chain = retriever.invoke(query)
    #make the relevant chunks of documents into a string
    context = format_documents(document_chain)
    # invoke chain with the string of the query and the context for the prompt
    result = chain.invoke({"input": query, "context":context}) 
    # print results
    print(result)

## MAIN USER INTERACTIONS START HERE######
user_input = input("What is your question?\n\n")
#Load the Documents
pdf = loadPDF()
#Chunk PDF
document_chunks = chunk_PDF(pdf)
#Create the vectors and store them in a Chroma database
vectors = build_or_load_vectors(document_chunks)
#Build the retriver that will determine relevant vectors
retriever = vectors.as_retriever(
    search_type = "similarity_score_threshold",
    search_kwargs={
        "k": 2,
        #This is the threashold for how relevant a chunk should be to be considered helpful
        "score_threshold": 0.5,
    }
)
while user_input.lower() != 'exit':
    ask(user_input, retriever)
    user_input = input("What is your question?\n\n")
